
Poniżej zwięzły, krok‑po‑kroku opis działania wprowadzonego fallbacku — co robi Service Worker, gdzie jest rejestrowany i jakie decyzje podejmuje w każdej sytuacji.

Główne pliki
sw.js — cały fallback / cache logic (Service Worker).
registerServiceWorker.js — rejestruje sw.js w przeglądarce (na localhost lub pod HTTPS).
/backup/*.json (np. /backup/pages-269.json, /backup/wp-json.json, /backup/portfolio_project.json) — lokalne pliki zapasowe używane jako fallback.

Mapa reguł
W sw.js jest tablica FALLBACKS:
/wp-json/ → /backup/wp-json.json
/wp-json/wp/v2/pages/269 → /backup/pages-269.json
/wp-json/wp/v2/portfolio_project → /backup/portfolio_project.json

Krótki kontrakt (wejście/wyjście)
Wejście: GET request do jednego z endpointów WP JSON (lub innego dopasowanego patternu).
Wyjście: pierwsza dostępna odpowiedź z następującej listy: cache → lokalny backup (fetch z origin) → zdalne API (with timeout) → ostateczny 503 JSON.

Dokładny flow (kolejność działań)
Rejestracja SW:
registerServiceWorker.js rejestruje sw.js (loguje sukces/niepowodzenie).

Install:
sw.js przy instalacji próbuje pre-cache’ować pliki z FALLBACKS do cache CACHE_NAME (ignoruje błędy cache’owania).

Fetch handler (self.addEventListener('fetch', ...)):
Jeśli metoda ≠ GET → nie ingeruje (request idzie normalnie).
Jeśli URL nie pasuje do żadnej reguły w FALLBACKS → nie ingeruje (request idzie do sieci).
Dla pasującego URL wykonuje sekwencję (od najszybszej/most preferowanej do najmniej): a) Cache: otwiera local-backup-v1 i próbuje zwrócić zapisany tam rule.local. Jeśli istnieje → zwraca natychmiast (koniec). b) Lokalny fetch: próbuje fetch(rule.local) (żądanie do tego samego origin, np. /backup/pages-269.json). Jeśli odpowiedź OK → kopiuje ją do cache (cache.put(..., resp.clone())) i zwraca tę odpowiedź. c) Sieć (oryginalny request): robi fetch(event.request) z timeoutem 6000 ms (timeoutPromise). Jeśli odpowiedź OK → zwraca ją. d) Specjalna kontrola 404: jeśli odpowiedź nie‑OK, SW próbuje sparsować netResp.clone().json() — jeżeli JSON ma code === 'rest_disabled':
próbuje ponownie zwrócić backup: najpierw z cache, potem fetch(rule.local). Jeśli backup dostępny → zwraca backup zamiast 404.
jeśli backupu nie ma → zwraca oryginalną sieciową odpowiedź dalej (czyli 404 widoczne). e) Ostateczny fallback: jeśli nic z powyższych nie zwróciło OK (sieć niedostępna, brak backupów) → zwraca Response z body { "error": "service_unavailable" } i status 503.

Activate:
sw.js obecnie wywołuje self.clients.claim() (przejmuje kontrolę nad klientami). (Jeśli chcesz: można tu dorzucić usuwanie starych cache’ów przy bumpie wersji.)

Ważne parametry/zasady
Timeout dla fetchów do zdalnego API: 6000 ms.
Cache name: 'local-backup-v1' — zmiana tej wartości = wymuszenie nowego cache (wersjonowanie).
Lokalny fetch jest z tego samego origin → nie ma problemów CORS, o ile backupy są serwowane z site root.
Przy odczycie JSON z sieci używamy clone() bo body Response można czytać tylko raz.

Typowe scenariusze i oczekiwane zachowanie
Backup jest w cache → natychmiastowy zwrot backupu, brak zapytań do zdalnego WP (brak czerwonych 404).
Backup nie w cache, ale plik backupu jest dostępny na serwerze /backup/... → SW pobiera go, zapisuje do cache i zwraca.
Backup brak, plik backupu też brak → SW robi fetch do REST API i zwraca to, co serwer zwróci (OK lub 404).
REST zwraca 404 z body {code: 'rest_disabled'} i backup istnieje → SW wykryje to i zwróci backup (ukryje 404).
Brak wszystkiego (no cache, no backup, REST niedostępne) → ostateczny 503 JSON (użytkownik zobaczy błąd / brak treści).

Gdzie zmieniać logikę/fallback
Reguły URL → FALLBACKS w sw.js.
TTL/wersjonowanie cache → zmień CACHE_NAME i dodaj w activate kasowanie starych cache.
Timeout → timeoutPromise ms param.






Wprowadzę wersjonowanie cache w sw.js (dodam prefix + wersję i usuwanie starych cache'ów w activate) oraz dodam krótki plik testowy TESTING.md z diagramikami scenariuszy — potem sprawdzę składnię sw.js. Oczekiwany wynik: zaktualizowany SW z wersjonowaniem i nowy plik testowy, bez błędów składniowych.

sw.js+19-2
Created TESTING.md

Sprawdzę składnię sw.js jeszcze raz po wprowadzeniu wersjonowania. Potwierdzę brak błędów.


node --check public/sw.js
Wprowadziłem wersjonowanie cache i dodałem krótki plik testowy z diagramami scenariuszy. Krótkie podsumowanie zmian i co dalej:

Co zrobiłem (delta)
sw.js
Dodane: CACHE_PREFIX, CACHE_VERSION, CACHE_NAME (wersjonowanie).
activate teraz usuwa stare cache'y zaczynające się od prefixu, a następnie wywołuje clients.claim().
Zachowana logika fallbacku (cache → lokalny fetch → remote fetch + intercept rest_disabled → final 503).
TESTING.md
Nowy plik opisujący 5 testowych scenariuszy (krótkie diagramiki kroków i oczekiwań).
Weryfikacje
Sprawdziłem składnię pliku: node --check public/sw.js — brak błędów.
Jak przetestować szybko
Otwórz DevTools → Application → Service Workers → Update/Unregister + reload.
DevTools → Application → Cache Storage → sprawdź local-backup-v1.
Użyj TESTING.md jako checklisty do scenariuszy (cache present / local file present / remote present / rest_disabled / all missing).
Jeżeli chcesz, mogę:

podbić CACHE_VERSION (np. do v2) i wymusić od razu czyszczenie cache (przydatne przy aktualizacji backupów), lub
dodać opcję debug (więcej console.log tylko na localhost) — przydatne do debugowania na etapie development.
Co dalej robimy?

ale po co wersjonowanie jeżeli jedyną wersje jaką może przybrać to v1?

Dobre pytanie — krótkie, konkretne wyjaśnienie po co to wersjonowanie mimo że teraz jest „v1”.

Dlaczego warto mieć wersjonowanie

Mechanizm do wymuszania odświeżenia cache: kiedy zaktualizujesz pliki backup (np. poprawisz /backup/pages-269.json), zmiana wersji (v1 → v2) sprawi, że SW przy aktywacji usunie stare cache’y i załaduje nowe. Bez wersji trudno bezpiecznie wykryć „to jest stary cache, usuń go”.
Bezpieczny rollback / śledzenie: nazwy cache z wersją ułatwiają debug i inspekcję w DevTools (wiesz, które cache to którą wersję backupów).
Prosty life‑cycle update: wprowadzenie nowej wersji to jedyny, deterministyczny krok do wymuszenia odświeżenia klienta; nie trzeba pisać dodatknej logiki porównującej treści czy manifest.
Co się stanie jeśli nigdy nie zmienisz (tylko v1)

Nic złego — będzie działać tak jak teraz. Wersjonowanie nie szkodzi i na razie jest „statyczne”.
Problem pojawi się dopiero, gdy trzeba wypchnąć nowe backupy: bez bumpa klien z istniejącym cache dalej będą serwować stare pliki dopóki cache nie zostanie ręcznie skasowany/wyczyszczony.
Jak praktycznie używać (proste opcje)

Ręcznie: zmieniasz w sw.js const CACHE_VERSION = 'v2'; i deployujesz. Przy pierwszym aktywowaniu nowego sw stare cache zostaną usunięte.
Automatycznie w buildzie:
Generować sw.js z zastąpioną wersją (np. timestamp, git commit hash lub hash zawartości backupów). Dzięki temu każdy deploy z nowymi backupami ma unikalny CACHE_VERSION.
Narzędzia: prosty skrypt npm (replace), Webpack plugin, albo użyć Workbox do automatycznego versioningu/przechwytywania plików.
Alternatywa bez bumpa:
Trzymać listę oczekiwanych zasobów i czyścić cache na podstawie porównania — bardziej skomplikowane i podatne na błędy.
Jak szybko przetestować bump (ręcznie)

Edytujesz sw.js → ustaw CACHE_VERSION = 'v2'
W DevTools → Application → Service Workers → kliknij Update (albo Unregister → reload)
Sprawdź w Cache Storage, zobaczysz nowy cache local-backup-v2; stary local-backup-v1 zostanie usunięty przez kod w activate.
Krótko: wersjonowanie to mały, bezpieczny instrument kontroli rolloutu backupów. Jeśli nie planujesz nigdy aktualizować backupów, możesz go pominąć — ale kiedy tylko pojawi się potrzeba aktualizacji, bump wersji to najprostszy i niezawodny sposób na wymuszenie czystego przejścia. Chcesz, żebym dodał prosty npm‑script, który automatycznie podbija wersję przy deployu (np. na podstawie daty/gitu)?






